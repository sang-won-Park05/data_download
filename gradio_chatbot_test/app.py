import gradio as gr
from typing import List
import os

from utils import get_env
import math
from retrieval import hybrid_search
from reranker import rerank
from rag import build_prompt, post_edit_instruction, SYSTEM_TONE
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOP_K = int(get_env("TOP_K", "5"))              # ë¦¬ë­í¬ í›„ ìµœì¢… ê°œìˆ˜
CANDIDATE_K = int(get_env("CANDIDATE_K", "50")) # ë¦¬ë­í¬ ì „ ë²¡í„° í›„ë³´
GPT_MODEL = get_env("GPT_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = get_env("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë‹µë³€ ìƒì„± í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_answer(message, history, top_k=TOP_K, candidate_k=CANDIDATE_K):
    query = message.strip()

    # 1) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    cand_docs, cand_ids, hybrid_scores = hybrid_search(
        query,
        candidate_k=candidate_k,
        top_k=top_k
    )

    # 2) ë¦¬ë­í¬
    reranked_docs, rerank_scores = rerank(query, cand_docs, top_k=top_k)

    # 3) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = build_prompt(query, reranked_docs)

    # 4) 1ì°¨ ì´ˆì•ˆ
    draft = client.chat.completions.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_TONE},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
    ).choices[0].message.content

    # 5) í›„ì²˜ë¦¬(ë§íˆ¬ ë³´ì •)
    polished = client.chat.completions.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_TONE},
            {"role": "user", "content": post_edit_instruction()},
            {"role": "user", "content": draft},
        ],
        temperature=0.2,
    ).choices[0].message.content

    debug = {
        "candidate_ids": cand_ids[:top_k],
        "hybrid_scores(top)": hybrid_scores[:top_k],
        "rerank_scores": rerank_scores,
        "contexts": reranked_docs,
    }

    return polished, debug

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Gradio UI êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks(title="Hybrid RAG Chat (Chroma + BM25 + Rerank)") as demo:
    gr.Markdown("## ğŸ” Hybrid RAG Chat\nChroma + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­ì»¤ + GPT í›„ì²˜ë¦¬")

    with gr.Row():
        # â”€â”€â”€ ì±—ë´‡ ì˜ì—­ â”€â”€â”€
        with gr.Column(scale=3):
            chat = gr.ChatInterface(
                fn=lambda msg, hist, tk, ck: generate_answer(msg, hist, tk, ck)[0],
                additional_inputs=[
                    gr.Slider(3, 10, value=TOP_K, step=1, label="ìµœì¢… Top-K"),
                    gr.Slider(10, 100, value=CANDIDATE_K, step=5, label="ë²¡í„° í›„ë³´ í’€(Candidate-K)"),
                ],
                type="messages",  # â† ì´ê±° ì¶”ê°€í•´ì„œ ê²½ê³  ì—†ì•°
                chatbot=gr.Chatbot(
                    height=480,
                    show_copy_button=True,
                    type="messages",  # â† Chatbotë„ ë§ì¶°ì¤Œ
                ),
            )

        # â”€â”€â”€ ë””ë²„ê·¸ / ê²€ìƒ‰ í™•ì¸ ì˜ì—­ â”€â”€â”€
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ§ª Debug & Retrieval View")
            query_in = gr.Textbox(label="í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬", placeholder="ì˜ˆ) ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì–´ë–¤ ë³‘ì› ê°€ë©´ ì¢‹ì„ê¹Œìš”?")
            tk_in = gr.Slider(3, 10, value=TOP_K, step=1, label="ìµœì¢… Top-K")
            ck_in = gr.Slider(10, 100, value=CANDIDATE_K, step=5, label="ë²¡í„° í›„ë³´ í’€(Candidate-K)")
            btn = gr.Button("ê²€ìƒ‰ & ë¦¬ë­í¬ ë³´ê¸°")

            # âš  ë„ˆ ê·¸ë¼ë””ì˜¤ ë²„ì „ì—ì„œëŠ” height / visible_rows ì•ˆ ë¨ â†’ row_countë¡œ
            ctx_out = gr.Dataframe(
                headers=["#", "Score", "Snippet"],
                value=[],              # ì´ˆê¸°ê°’ ë¹ˆ í‘œ
                row_count=10,          # í‘œì‹œí•  ì¤„ ìˆ˜
                col_count=3,
                interactive=False,
            )
            metrics_json = gr.JSON(label="Reranker Metrics (vs Hybrid): @5")
            raw_json = gr.JSON(label="Raw debug")

            def inspect(query, tk, ck):
                # í•˜ì´ë¸Œë¦¬ë“œë¡œ ë¨¼ì € 'í›„ë³´ í’€(ck)'ê¹Œì§€ í¬ê²Œ ë½‘ê³  (ì ìˆ˜ í¬í•¨)
                docs, ids, hybrid_scores = hybrid_search(
                    query,
                    candidate_k=int(ck),
                    top_k=int(ck)
                )
                # ê·¸ ìœ„ì— ë¦¬ë­í¬í•˜ì—¬ ìµœì¢… tkë§Œí¼ ì„ íƒ
                rer_docs, r_scores = rerank(query, docs, top_k=int(tk))

                # í‘œì— ë„£ì„ ë°ì´í„° ë§Œë“¤ê¸°
                rows = []
                for i, (d, s) in enumerate(zip(rer_docs, r_scores), start=1):
                    rows.append([
                        i,
                        round(float(s), 4),
                        (d[:250] + "...") if len(d) > 250 else d,
                    ])

                # â”€â”€ ë¦¬ë­ì»¤ í‰ê°€ ì§€í‘œ(@5): í•˜ì´ë¸Œë¦¬ë“œ ìƒìœ„ì™€ì˜ í•©ì¹˜ë„ ê¸°ì¤€ â”€â”€
                def compute_metrics_at5(candidates, cand_scores, reranked, k=5):
                    k = min(k, len(candidates))
                    if k == 0:
                        return {
                            "MRR@5": 0.0,
                            "nDCG@5": 0.0,
                            "Recall@5": 0.0,
                            "Precision@5": 0.0,
                        }

                    # í•˜ì´ë¸Œë¦¬ë“œ ì •ë ¬ì€ ì´ë¯¸ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ â†’ ìƒìœ„ kë¥¼ ì´ìƒ(ideal)ìœ¼ë¡œ ì‚¬ìš©
                    ideal_docs = candidates[:k]

                    # ì ìˆ˜ ë§µ (nDCGìš©, ì ìˆ˜ëŠ” 0~1 ì •ê·œí™”ëœ ê°’)
                    score_map = {doc: float(s) for doc, s in zip(candidates, cand_scores)}

                    # rerank ìƒìœ„ k
                    rr_k = reranked[:k]

                    # Precision/Recall@k (ideal@kê³¼ì˜ êµì§‘í•© ê¸°ë°˜)
                    ideal_set = set(ideal_docs)
                    rr_set = set(rr_k)
                    inter = len(ideal_set.intersection(rr_set))
                    precision = inter / max(1, len(rr_k))
                    recall = inter / max(1, len(ideal_docs))

                    # MRR@k: ì²« ê´€ë ¨ë¬¸ì„œ(ideal@kì— ì†í•¨)ì˜ ì—­ìˆœìœ„
                    mrr = 0.0
                    for rank, d in enumerate(rr_k, start=1):
                        if d in ideal_set:
                            mrr = 1.0 / rank
                            break

                    # nDCG@k: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¥¼ graded relevanceë¡œ ì‚¬ìš©
                    def dcg(docs_seq):
                        total = 0.0
                        for i, d in enumerate(docs_seq[:k], start=1):
                            rel = score_map.get(d, 0.0)
                            total += (2 ** rel - 1.0) / math.log2(i + 1)
                        return total

                    idcg = dcg(ideal_docs)
                    ndcg = (dcg(rr_k) / idcg) if idcg > 0 else 0.0

                    return {
                        "MRR@5": round(mrr, 4),
                        "nDCG@5": round(ndcg, 4),
                        "Recall@5": round(recall, 4),
                        "Precision@5": round(precision, 4),
                    }

                metrics = compute_metrics_at5(docs, hybrid_scores, rer_docs, k=5)

                # generate_answer ì˜ debugë„ ê°™ì´
                _, debug = generate_answer(query, [], int(tk), int(ck))
                return rows, metrics, debug

            btn.click(fn=inspect, inputs=[query_in, tk_in, ck_in], outputs=[ctx_out, metrics_json, raw_json])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
